{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import einops\n",
    "from dataclasses import dataclass\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from collections import defaultdict\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the configuration for the model we will be building: Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Here is the configuration for the model we will be building: {cfg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        mean = t.mean(residual, dim=-1, keepdim=True)\n",
    "        var = t.var(residual, unbiased=False, dim=-1, keepdim=True)\n",
    "        eps = cfg.layer_norm_eps\n",
    "\n",
    "        y = (residual - mean) / t.sqrt(var + eps)\n",
    "        return (self.w * y) + self.b\n",
    "    \n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range) # init the weights sampling from a normal distribution\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "      return self.W_E[tokens]\n",
    "  \n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "      num_batches, seq_len = tokens.shape\n",
    "      return self.W_pos[:seq_len].repeat(num_batches, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # get keys, queries, and values\n",
    "\n",
    "        keys = einops.einsum(\n",
    "            normalized_resid_pre, self.W_K,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\") + self.b_K\n",
    "\n",
    "        queries = einops.einsum(\n",
    "            normalized_resid_pre, self.W_Q,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\") + self.b_Q\n",
    "\n",
    "        values = einops.einsum(\n",
    "            normalized_resid_pre, self.W_V,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\") + self.b_V\n",
    "\n",
    "        attn_scores = einops.einsum(\n",
    "            queries, keys,\n",
    "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\")\n",
    "\n",
    "        attn_scores = attn_scores / cfg.d_head ** 0.5\n",
    "\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "\n",
    "        attn_probs = t.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        weighted = einops.einsum(\n",
    "            values, attn_probs,\n",
    "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
    "        )\n",
    "\n",
    "        out = einops.einsum(\n",
    "            weighted, self.W_O,\n",
    "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
    "        ) + self.b_O\n",
    "\n",
    "        return out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE) # leaves us with a lower triangular matrix\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        pre = einops.einsum(\n",
    "            normalized_resid_mid, self.W_in,\n",
    "            \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "        ) + self.b_in\n",
    "\n",
    "        post = gelu_new(pre)\n",
    "\n",
    "        mlp_out = einops.einsum(\n",
    "            post, self.W_out,\n",
    "            \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "        ) + self.b_out\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return einops.einsum(normalized_resid_final, self.W_U, \"batch position d_model, d_model d_vocab -> batch position d_vocab\") + self.b_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual_stream = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "          residual_stream = block(residual_stream)\n",
    "        residual_stream = self.ln_final(residual_stream)\n",
    "        out = self.unembed(residual_stream)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_gpt2 = Transformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "tokens = t.tensor([reference_gpt2.tokenizer.encode(\"Hello, my name is\")])\n",
    "\n",
    "demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs():\n",
    "\tbatch_size = 16\n",
    "\tepochs = 20\n",
    "\tmax_steps_per_epoch = 200\n",
    "\tlr = 1e-3\n",
    "\tweight_decay = 1e-2\n",
    "\twandb_project: Optional[str] = \"jackson-transformer\"\n",
    "\twandb_name: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10k pile dataset\n",
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "\n",
    "tokenized_dataset = tokenize_and_concatenate(dataset, reference_gpt2.tokenizer, streaming=False, max_length=cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "\n",
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(dataset_dict[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_dict[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "\tdef __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = model\n",
    "\t\tself.args = args\n",
    "\t\tself.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\t\tself.step = 0\n",
    "\n",
    "\n",
    "\tdef training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "\t\t'''\n",
    "\t\tCalculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "\t\tRemember that `batch` is a dictionary with the single key 'tokens'.\n",
    "\t\t'''\n",
    "\t\ttok = batch['tokens'].to(device)\n",
    "\t\tlogits = self.model(tok) # get outputs\n",
    "\t\t# compute loss\n",
    "\t\tloss = -get_log_probs(logits, tok).mean()\n",
    "\t\tloss.backward() # compute grad\n",
    "\t\tself.optimizer.step() # update grad\n",
    "\t\tself.optimizer.zero_grad() # zero out the grad to prevent accumulation\n",
    "\t\tself.step += 1 # step up\n",
    "\t\twandb.log({\"train_loss\": loss}, step=self.step) # log it\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "\t\t'''\n",
    "\t\tCalculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
    "\t\tis correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
    "\t\tthe whole validation set).\n",
    "\t\t'''\n",
    "\t\ttok = batch['tokens'].to(device)\n",
    "\t\tlogits = self.model(tok)[:, :-1] # get outputs, exclude the last prediction\n",
    "\t\tpredicts = logits.argmax(dim=-1)\n",
    "\n",
    "\t\tcorrect_predicts = (predicts == tok[:, 1:]).flatten()\n",
    "\t\treturn correct_predicts # calculating acc will be in training step\n",
    "\n",
    "\n",
    "\tdef train(self):\n",
    "\t\t'''\n",
    "\t\tTrains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "\t\tfor each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "\t\t'''\n",
    "\t\twandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "\t\taccuracy = np.nan\n",
    "\n",
    "\t\tpb = tqdm(total = self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "\t\tfor epoch in range(self.args.epochs):\n",
    "\t\t\tfor i, batch in enumerate(self.train_loader()):\n",
    "\t\t\t\tloss = self.training_step(batch)\n",
    "\t\t\t\tpb.update()\n",
    "\t\t\t\tpb.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
    "\t\t\t\tif i >= self.args.max_steps_per_epoch:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\tcorrect_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
    "\t\t\taccuracy = correct_predictions.float().mean().item()\n",
    "\t\t\twandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "\t\twandb.finish()\n",
    "\n",
    "\tdef train_loader(self) -> DataLoader:\n",
    "\t\t'''Returns train loader (as in code above).'''\n",
    "\t\treturn DataLoader(dataset_dict[\"train\"], batch_size=self.args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "\tdef test_loader(self) -> DataLoader:\n",
    "\t\t'''Returns test loader (as in code above).'''\n",
    "\t\treturn DataLoader(dataset_dict[\"test\"], batch_size=self.args.batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoTransformer(cfg)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
